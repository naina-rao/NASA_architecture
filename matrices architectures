import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter
import numpy as np
import random
from numpy.random import seed
import os, sys, math
from PIL import Image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import BatchNormalization

seed_value = 7
os.environ['PYTHONASHSEED']=str(seed_value)
random.seed(seed_value)
np.random.seed(seed_value)
tf.random.set_seed(seed_value)
os.environ['PYTHONASHSEED']=str(seed)

#You'll have to change these to work with your machines
datasetRegion = 'SA' # input("Region: ")

NPZPath = '/content/drive/MyDrive/GRACE Team 2021/matrixData/'
NPZFname = 'SA_monthly_trainingTesting_numpy.npz'
imageFname = 'SA_AOHISMonthlyTraining_GRACEmonthlyTest_SEESArch_numpy_052021.png'
#NPZFname = '{}_monthly_trainingTesting_numpy.npz'.format(datasetRegion)
#imageFname = '{}_AOHISMonthlyTraining_GRACEmonthlyTest_SEESArch_numpy_052021.png'.format(datasetRegion)
resultsDir = '/content/drive/MyDrive/Corinne Test Results/'
modelType = 'controlModel'

with np.load(NPZPath+NPZFname) as data:
	trainData = data['trainingData']
	trainLabels = data['trainingLabels']
	print(trainData)
	testData = data['testingData']
	testLabels = data['testingLabels']
  #print(trainData)

  #fileTest = np.load(data['trainingData'])
  #print(fileTest)
  
  #.npz files do weird things to data size, shape, & type- reformatting so works with tensorflow environment
print('shape of trainData')
print(np.shape(trainData))
mSize = trainData.shape[1] * trainData.shape[2]
numTrainImages = trainData.shape[0]
numTestImages = testData.shape[0]
#trainData = trainData.reshape(trainData.shape[0], trainData.shape[1] * trainData.shape[2]).astype("float32")
#testData = testData.reshape(testData.shape[0], testData.shape[1] * testData.shape[2]).astype("float32")
print('shape of trainData')
print(np.shape(trainData))
print(trainData.shape[0])
print(trainData.shape[1])
print(trainData.shape[2])

all_train_images = []
for i in range(numTrainImages):
	img = trainData[i,:,:]
	width = img.shape[0]
	height = img.shape[1]
	img = img.reshape([width, height, 1]).astype("float32")
	all_train_images.append(img)


all_test_images = []
for i in range(numTestImages):
	img = testData[i,:,:]
	width = img.shape[0]
	height = img.shape[1]
	img = img.reshape([width, height, 1]).astype("float32")
	all_test_images.append(img)
  
  #sys.exit()
trainLabels = trainLabels.astype("float32")
testLabels = testLabels.astype("float32")

#print(max(trainLabels), min(trainLabels))
trainLabels = trainLabels 
testLabels = testLabels 
print(max(trainLabels), min(trainLabels)) #making sure labels are between 0 & 11 for monthly sorting


#makes categories out of the labels 
trainLabels = tf.keras.utils.to_categorical(trainLabels,12)
testLabels = tf.keras.utils.to_categorical(testLabels,12)

#train_dataset = tf.data.Dataset.from_tensor_slices((trainData, trainLabels))
#test_dataset = tf.data.Dataset.from_tensor_slices((testData, testLabels))

# train_dataset = tf.data.Dataset.from_tensor_slices((all_train_images, trainLabels))
# test_dataset = tf.data.Dataset.from_tensor_slices((all_test_images, testLabels))
# print(' train_dataset')
# print(train_dataset)

# batchSizeList = [2, 4, 8, 16, 32]
# batchSize = 1
# train_dataset = train_dataset.batch(batchSize)
# test_dataset = test_dataset.batch(batchSize)

#Model Architecture  ###Control w/ re-arranged preprocessing & NO Batch Norm ########YESSSSS
def model1():
  model = models.Sequential()
  model.add(BatchNormalization())#
  #model.add(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(trainData.shape[1], trainData.shape[2], 1)))
  model.add(layers.Conv2D(16, (2, 2), padding='same', activation='relu', input_shape=(trainData.shape[1], trainData.shape[2], 1)))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))

  model.add(layers.Flatten())
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(12, activation='softmax'))
  model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                metrics=['accuracy'], experimental_run_tf_function=False)
  return model

#Model Architecture #Stride/downsampling & 1 batchNorm & GAP #####YESSS
def model2():
  model = models.Sequential()
  model.add(BatchNormalization())#1
  #model.add(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(trainData.shape[1], trainData.shape[2], 1)))
  model.add(layers.Conv2D(16, (2, 2),strides=2,  activation='relu', input_shape=(trainData.shape[1], trainData.shape[2], 1)))
  model.add(BatchNormalization())#2
  model.add(layers.MaxPooling2D((2, 2)))#1
  model.add(layers.Conv2D(32, (3, 3),strides=2,  activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))#1
  model.add(layers.Conv2D(64, (3, 3),strides=2,  activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))#1
  model.add(layers.Conv2D(128, (3, 3),strides=2, padding='same', activation='relu'))

  model.add(layers.GlobalAveragePooling2D())
  model.add(layers.Dense(12, activation='softmax'))

  model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                metrics=['accuracy'], experimental_run_tf_function=False)
  return model

#Model Architecture #Stride/downsampling & 1 batchNorm & Orig Flatten & FCL #####YESSSSSSS
def model3():
  model = models.Sequential()
  model.add(BatchNormalization())#1
  #model.add(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(trainData.shape[1], trainData.shape[2], 1)))
  model.add(layers.Conv2D(16, (2, 2),strides=2,  activation='relu', input_shape=(trainData.shape[1], trainData.shape[2], 1)))
  model.add(BatchNormalization())#2
  model.add(layers.MaxPooling2D((2, 2)))#1
  model.add(layers.Conv2D(32, (3, 3),strides=2,  activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))#1
  model.add(layers.Conv2D(64, (3, 3),strides=2,  activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))#1
  model.add(layers.Conv2D(128, (3, 3),strides=2, padding='same', activation='relu'))

  model.add(layers.Flatten())
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(12, activation='softmax'))

  model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                metrics=['accuracy'], experimental_run_tf_function=False)
  return model

train_dataset = tf.data.Dataset.from_tensor_slices((all_train_images, trainLabels))
test_dataset = tf.data.Dataset.from_tensor_slices((all_test_images, testLabels))
print(' train_dataset')
print(train_dataset)

batchSizeList = [2, 4, 8, 16, 32]
#batchSize = 1
numEpochs=20

#for index, size in enumerate(batchSizeList):
train_dataset = train_dataset.batch(batchSizeList[1])
test_dataset = test_dataset.batch(batchSizeList[1])
model = model2() # change for the type of test trying to run
model.summary()
history = model.fit(train_dataset, epochs=numEpochs, validation_data=test_dataset, batch_size = batchSizeList[0])

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label = 'Testing Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
# plt.savefig(resultsDir + '{}Plot{}.png'.format(modelType, index)) # Saving model
plt.show()

# f.write(str(index) + ". Batch Size: " + str(size))
# f.write("Training Accuracy: " + str(history.history['accuracy']))
# f.write("Testing Accuracy: " + str(history.history['val_accuracy']))

# f.close()

print(max(history.history['accuracy']))
print(max(history.history['val_accuracy']))
